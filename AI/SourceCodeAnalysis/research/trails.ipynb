{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo \n",
    "from langchain.text_splitter import Language,RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import  ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Python\\\\New folder\\\\AIprojects\\\\GenAIProjects\\\\AI\\\\SourceCodeAnalysis\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo 'e:\\\\Python\\\\New folder\\\\AIprojects\\\\GenAIProjects\\\\AI\\\\SourceCodeAnalysis\\\\research\\\\test_repo\\\\.git'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path=\"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/kpayyam1998/End-to-End-MLproject.git\",to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=GenericLoader.from_filesystem(repo_path+\"src\",\n",
    "                              glob=\"**/*\",\n",
    "                              suffixes=[\".py\"],\n",
    "                              parser=LanguageParser(language=Language.PYTHON,parser_threshold=500)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\"\n",
      "    Logger which is nothing but whenever excuting our program it will track everything \n",
      "    Even custom excepton error also we can log\n",
      "\n",
      "    If want to learn more gothrouth document\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "import os\n",
      "import logging\n",
      "\n",
      "from datetime import datetime\n",
      "\n",
      "LOGFILE=f\"{datetime.now().strftime('%m_%d_%y_%H_%M_%S')}.log\" # Getting current date time with extension  .log filename\n",
      "log_path=os.path.join(os.getcwd(),\"logs\",LOGFILE) # combine with path and file name\n",
      "os.makedirs(log_path,exist_ok=True) # crating directory\n",
      "\n",
      "\n",
      "LOGS_FILE_PATH=os.path.join(log_path,LOGFILE) # full path and file name\n",
      "\n",
      "logging.basicConfig(\n",
      "    filename=LOGS_FILE_PATH,\n",
      "\n",
      "    format=\"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s -%(message)s\",\n",
      "    level=logging.INFO # this line going to write my exitre msg\n",
      "\n",
      ")\n",
      "\n",
      "# if __name__==\"__main__\":\n",
      "#     logging.info(\"Logging started...\")\n"
     ]
    }
   ],
   "source": [
    "print(documents[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_splitter=RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                             chunk_size=2000,\n",
    "                                             chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=document_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents),len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge base (Vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Python\\New folder\\AIprojects\\GenAIProjects\\env\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectordb=Chroma.from_documents(text,embedding=embeddings,persist_directory=\"./data\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationSummaryMemory(human_prefix='Human', ai_prefix='AI', llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), prompt=PromptTemplate(input_variables=['summary', 'new_lines'], output_parser=None, partial_variables={}, template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', template_format='f-string', validate_template=True), summary_message_cls=<class 'langchain.schema.messages.SystemMessage'>, chat_memory=ChatMessageHistory(messages=[]), output_key=None, input_key=None, return_messages=True, buffer='', memory_key='chat_history')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=ConversationalRetrievalChain.from_llm(llm,retriever=vectordb.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":3}),memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-WmjsJX6bpNkeyN8frVrFJDIL on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-WmjsJX6bpNkeyN8frVrFJDIL on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for gpt-3.5-turbo in organization org-WmjsJX6bpNkeyN8frVrFJDIL on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing..\n"
     ]
    }
   ],
   "source": [
    "question=\"what is data_transformation class?\"\n",
    "response=qa(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is data_transformation class?',\n",
       " 'chat_history': [SystemMessage(content='The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human inquires about the data transformation class, and the AI responds that the class for data transformation is `DataTransformation`.', additional_kwargs={})],\n",
       " 'answer': 'The `DataTransformation` class is responsible for data transformation tasks based on the data provided. It handles preprocessing of both numerical and categorical columns, including imputing missing values, standard scaling, and one-hot encoding. It creates a pipeline that combines these transformations for numerical and categorical data. The class also provides methods to initiate data transformation, read data files, and apply the preprocessing object on the training and testing data frames.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `DataTransformation` class is responsible for data transformation tasks based on the data provided. It handles preprocessing of both numerical and categorical columns, including imputing missing values, standard scaling, and one-hot encoding. It creates a pipeline that combines these transformations for numerical and categorical data. The class also provides methods to initiate data transformation, read data files, and apply the preprocessing object on the training and testing data frames.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project has its successfully giving the response based on our prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
